{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with ISO Fortran\n",
    "===\n",
    "\n",
    "# Lab 2: 2D Unsteady Heat Equation\n",
    "\n",
    "The following cell loads the visualization scripts, you'll need to load it to be able to call `visualize()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run vis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: multi-gpu heat equation with `do concurrent`\n",
    "\n",
    "In this exercise, we provide you with a mini-application for the two-dimensional unsteady heat equation.\n",
    "It is written using Fortran and parallelized for distributed memory with MPI.\n",
    "\n",
    "The goal of this first exercise is to parallelize each MPI rank using `do concurrent`, so that we end up with a mini-application that uses MPI for distributed memory across nodes, but hybrid parallelization for CPU cores, or GPUs, within a node. All in portable standard-compliant ISO Fortran 2023.\n",
    "\n",
    "The compilation commands below compile the template of this exercise [exercise1.f90](./exercise1.f90) for CPUs and GPUs. The template produces correct results, albeit sequentially; start by running it (see below). It contains many `! TODO`s. The only code that needs to be changed is the code around the `! TODO`s; if a `subroutine` or `function` does not contain any `! TODO`s, there is nothing to do (hah).\n",
    "\n",
    "It is recommended to start by parallelizing the `initial_condition`, since it is very similar to the previous DAXPY lab:\n",
    "\n",
    "```fortran\n",
    "  ! TODO: parallelize with do-concurrent\n",
    "  do x = 1,p%nx\n",
    "    do y = 1,p%ny\n",
    "      u_old(x, y) = 0.\n",
    "      u_new(x, y) = 0.\n",
    "    end do\n",
    "  end do\n",
    "```\n",
    "\n",
    "Then, continue by parellizing the loop that applies the stencil to all elements of a grid tile:\n",
    "\n",
    "```fortran\n",
    "  ! TODO: parallelize with do-concurrent\n",
    "  do x = g%x_start,g%x_end\n",
    "    do y = g%y_start, g%y_end\n",
    "      ! Boundary conditions\n",
    "      ! ...\n",
    "      u_new(x, y) = stencil(u_old, x, y, p)\n",
    "      energy = energy + u_new(x, y) * p%dx * p%dx\n",
    "    end do\n",
    "   end do\n",
    "```\n",
    "\n",
    "This loop:\n",
    "\n",
    "* performs a reduction for the `energy`, and it requires using the `reduce(op:variable)` locality-specifier for correctness:\n",
    "  ```fortran\n",
    "  do concurrent (i = 1:n) reduce(+:s)\n",
    "     ...\n",
    "  end do\n",
    "  ```\n",
    "* calls a `function` (`stencil`):\n",
    "  * All functions called from `do concurrent` loops must be `pure`, so we'll need to make that function and any function it calls `pure` (there are `! TODO`s for that).\n",
    "  * A bug in `nvfortran` 23.5 currently requires annotating functions called by `do concurrent` loops as \"device\" routines manually. The simplest way to do that is to use OpenAcc `acc routine seq` clause to annotate them:\n",
    "  ```fortran\n",
    "  function stencil(u_old, x, y, p) result(o)\n",
    "    !$acc routine seq\n",
    "    !...\n",
    "  end function\n",
    "  ```\n",
    "\n",
    "The following compilation commands may be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda exercise1.f90 -o heat\n",
    "!mpirun -np 1 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore exercise1.f90 -o heat\n",
    "!mpirun -np 1 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Exercise 1\n",
    "\n",
    "The [solutions/exercise1.f90](./solutions/exercise1.f90) can be tested with the following compilation commands. The problem size we've picked is very small, to see the benefits of GPU performance pick a grid-size of at least 8192x8192 or 16384x8192, but be mindful of other students if you are sharing computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda solutions/exercise1.f90 -o heat\n",
    "!mpirun -np 1 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore solutions/exercise1.f90 -o heat\n",
    "!mpirun -np 1 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Asynchrony\n",
    "\n",
    "This mini-application uses three kernels:\n",
    "* `inner`: for the inner tile of the grid, which does not depend on data from neighboring MPI ranks\n",
    "* `prev` and `next`: for the boundaries of the grid, whose computation depends on exchanging a column with neighboring MPI ranks\n",
    "\n",
    "NVIDIA is actively contributing to the standardization of asynchrony in ISO Fortran, e.g, [see this issue](https://github.com/j3-fortran/fortran_proposals/issues/271).\n",
    "\n",
    "In this exercise, we use OpenAcc `acc kernels async` clause to make the `do concurrent` loops asynchronous, waiting on them with the `acc wait` clause:\n",
    "\n",
    "```fortran\n",
    "!$acc kernels async\n",
    "do concurrent (...)\n",
    "  ! ...\n",
    "end do\n",
    "!$acc end kernels\n",
    "\n",
    "!$acc kernels async\n",
    "do concurrent (...)\n",
    "  ! ...\n",
    "end do\n",
    "!$acc end kernels\n",
    "\n",
    "!$acc wait\n",
    "```\n",
    "\n",
    "The [exercise2.f90](./exercise2.f90) template provides a starting point with a few `! TODO`s to achieve that using the following compilation commands.\n",
    "\n",
    "In this exercise, we use a single MPI rank to fully overlap the three kernels.\n",
    "In the next exercise, we will see how to overlap the kernels when host computation is involved during the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda exercise2.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore exercise2.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Exercise 2\n",
    "\n",
    "The [solutions/exercise2.f90](./solutions/exercise2.f90) can be tested with the followign compilation commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda solutions/exercise2.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore solutions/exercise2.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Overlapping communication and computation\n",
    "\n",
    "In the previous exercise we saw how to overlap three concurrent `do concurrent` loops using `acc kernels async`. \n",
    "\n",
    "In this exercise, we will fully overlap the computation and communication using OpenMP tasks:\n",
    "\n",
    "```fortran\n",
    "!$omp parallel\n",
    "!$omp master\n",
    "\n",
    "!$omp task\n",
    "do concurrent (...)\n",
    "  ! ...\n",
    "end do\n",
    "!$omp end task\n",
    "\n",
    "!$omp task\n",
    "do concurrent (...)\n",
    "  ! ...\n",
    "end do\n",
    "!$omp end task\n",
    "    \n",
    "!$omp end master\n",
    "!$omp end parallel\n",
    "```\n",
    "\n",
    "Follow the `! TODOs` in the [exercise3.f90](./exercise3.f90) template using the following compilation commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda exercise3.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore exercise3.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Exercise 3\n",
    "\n",
    "The [solutions/exercise3.f90](./solutions/exercise3.f90) can be tested using the following compilation commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=gpu -cuda solutions/exercise3.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm heat || true\n",
    "!mpifort -Wall -Ofast -stdpar=multicore solutions/exercise3.f90 -o heat\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll profile the mini-application with [NSight Systems](https://developer.nvidia.com/nsight-systems) to verify that the communication and computations are overlapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --trace=mpi,cuda,openmp,openacc,osrt --force-overwrite=true -o report.nsys-rep  mpirun -np 2 ./heat 16384 8192 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
