{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with ISO Fortran\n",
    "===\n",
    "\n",
    "# Lab 1: DAXPY\n",
    "\n",
    "In this tutorial we will familiarize ourselves with the Fortran DO CONCURRENT feature by implementing Double-precision AX Plus Y (DAXPY): $A \\cdot X + Y$, one of the main functions in the standard Basic Linear Algebra Subroutines (BLAS) library.\n",
    "\n",
    "The operation is a combination of scalar multiplication and vector adition. It takes two vectors of 64-bit floats, `x` and `y` and a scalar value `a`.\n",
    "It multiplies each element `x(i)` by `a` and adds the result to `y(i)`.\n",
    "\n",
    "A sequential working implementation is provided in [daxpy.f90].\n",
    "Please take 2-3 minutes to skim through it.\n",
    "\n",
    "## Validating solutions\n",
    "\n",
    "For all the exercises, we assume that initially the values are `x(i) = i` and `y(i) = 2`.\n",
    "The `check` function then verifies the effect of applying `daxpy` to these two vectors.\n",
    "We will run this check always once.\n",
    "\n",
    "## Sequential implementation\n",
    "\n",
    "The \"core\" of the sequential implementation provided in [daxpy.f90] is split into two parts:\n",
    "\n",
    "```fortran\n",
    "! Intialize vectors `x` and `y`: raw loop sequential version\n",
    "do i = 1, n\n",
    "  x(i)  = i\n",
    "  y(i)  = 2.\n",
    "enddo\n",
    "! daxpy\n",
    "subroutine daxpy(x, y, n, a)\n",
    "  real(kind=8), dimension(:) :: x, y\n",
    "  real(kind=8) :: a\n",
    "  integer :: n, i  \n",
    "  ! TODO: use do concurrent here\n",
    "  do i = 1, n\n",
    "    y(i) = y(i) + a * x(i)\n",
    "  enddo  \n",
    "end subroutine \n",
    "```\n",
    "\n",
    "We initialize the vectors to the `x(i) = 1` and `y(i) = i` expressions covered above for testing purposes.\n",
    "\n",
    "The `daxpy` subroutine implements a loop over all vector elements, reading from both `x` and `y` and writing the solution to `y`.\n",
    "\n",
    "[daxpy.f90]: ./daxpy.f90\n",
    "\n",
    "## Exercise 1 - `do concurrent`\n",
    "\n",
    "Let's start by compiling and running the [exercise1.f90](./exercise1.f90) template; the binary options are `./daxpy nx niterations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Wall -Wextra exercise1.f90 -o daxpy\n",
    "!./daxpy 1000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortran 2008 added support for `do concurrent` to express that loop iterations are independent from each other, and therefore safe for the implementation to implicitly parallelize them.\n",
    "\n",
    "For example, the following two dimensional loop:\n",
    "\n",
    "```fortran\n",
    "do i = 1,ni\n",
    "  do j = 1,nj\n",
    "     if (A(i, j) >= 0) then\n",
    "       cycle\n",
    "     end if\n",
    "     ...\n",
    "  end do\n",
    "end do\n",
    "```\n",
    "\n",
    "is rewritten with `do concurrent` as:\n",
    "\n",
    "```fortran\n",
    "do concurrent (i = 1:ni, j = 1:nj, A(i, j) < 0)\n",
    "  ...\n",
    "end do\n",
    "```\n",
    "\n",
    "In this exercise, you'll modify the sections indicated by `! TODO` comments in the [exercise1.f90](./exercise1.f90) template to parallelize the two loops using `do concurrent` as shown above.\n",
    "\n",
    "The following compilation commands can be used to test the implementation with `gfortran` and `nvfortran`.\n",
    "For both compilers, we enable optimizations using the `-Ofast` flag. \n",
    "For `nvfortran`, the `-stdpar=multicore` and `-stdpar=gpu` options auto-parallelize do-concurrent loops on CPUs and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!gfortran -Ofast -Wall -Wextra exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra exercise1.f90 -o daxpy\n",
    "!./daxpy 1000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1\n",
    "\n",
    "The solution is available in [solutions/exercise1.f90](./solutions/exercise1.f90):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!gfortran -Ofast -Wall -Wextra solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - `do concurrent` locality specifiers\n",
    "\n",
    "Fortran 2018 introduced the following locality specifiers:\n",
    "* `default(none)`: requires every variable used in the loop to have an explicit locality specifier except for loop indices.\n",
    "* `shared`: different iterations of the loop share the same variable memory\n",
    "* `local`: every _iteration_ of the loop gets an uninitialized private storage for the variable\n",
    "* `local_init`: `local` initialized with the variable's value outside the loop\n",
    "\n",
    "Fortran 2023 introduces the following locality specifiers:\n",
    "\n",
    "* `reduce(op:variable)` (e.g. `reduce(+:sum)`): different iterations share the same variable memory and reduce to it with the given operation.\n",
    "\n",
    "The locality specifiers are specified as part of the `do concurrent` loop:\n",
    "\n",
    "```fortran\n",
    "integer :: a, b, c\n",
    "\n",
    "do concurrent(i = 1:ni, j = 1:nj) default(none) shared(a) local_init(b, c)\n",
    "  ...\n",
    "end do\n",
    "```\n",
    "\n",
    "Multiple variables can be specified within the same specifier, as shown by the usage of `local_init(b, c)` which specifies the locality for both variables.\n",
    "\n",
    "In this exercise, you'll modify the sections indicated by `! TODO` comments in the [exercise2.f90](./exercise2.f90) template to:\n",
    "* add the `default(none)` specifier to the two `do concurrent` loops\n",
    "* add the remaining locality specifiers for all other variables.\n",
    "\n",
    "You can compile the [exercise2.f90](./exercise2.f90) template with the following compilation commands.\n",
    "\n",
    "Note: since `gfortran` does not support Fortran 2018 locality specifiers yet, we use `nvfortran` for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions - Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm daxpy || true\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
