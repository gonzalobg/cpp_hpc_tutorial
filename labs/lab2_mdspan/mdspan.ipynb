{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60731469-f356-4059-80e5-4f7a7954bd45",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with Standard C++\n",
    "===\n",
    "\n",
    "# Lab 2 Optional Exercises: `std::mdspan`\n",
    "\n",
    "This notebook contains the optional exercises of the Heat Equation lab. \n",
    "In these exercises, we will focus on porting the solution of Exercise 1 to use `std::mdspan`.\n",
    "\n",
    "The solution of Exercise 1 is available here: [`solutions/exercise1.cpp`](./solutions/exercise1.cpp), and the apply stencil function includes code like this:\n",
    "\n",
    "```c++\n",
    "double stencil(double *u_new, double *u_old, long x, long y, parameters p) {\n",
    "  auto idx = [=](auto x, auto y) { \n",
    "      // Index into the memory using row-major order:\n",
    "      assert(x >= 0 && x < (p.nx + 2));\n",
    "      assert(y >= 0 && y < p.ny);\n",
    "      return x * p.ny + y;\n",
    "  };\n",
    "  // Apply boundary conditions:\n",
    "  if (y == 1) {\n",
    "    u_old[idx(x, y - 1)] = 0;\n",
    "  }\n",
    "  // ...\n",
    "}\n",
    "```\n",
    "\n",
    "Notice how in this implementation, we are hardcoding the memory layout of our data structures into the `idx` function, which we then use to perform multi-dimensional access like this: `u_old[idx(x, y - 1)]`.\n",
    "\n",
    "In these optional exercises, we will update this solution to be generic over the memory layout of our data-structures, so that we can index it directly using C++23's multi-dimensional operator[]: \n",
    "\n",
    "```c++\n",
    "using grid_t = std::mdspan<...>;\n",
    "\n",
    "double stencil(grid_t u_new, grid_t u_old, long x, long y, parameters p) {\n",
    "  // Apply boundary conditions:\n",
    "  if (y == 1) {\n",
    "    u_old[x, y - 1] = 0;\n",
    "  }\n",
    "  // ...\n",
    "}\n",
    "```\n",
    "\n",
    "These optional exercises are structured as follows:\n",
    "\n",
    "- [`exercise1o.cpp`](./exercise1o.cpp): introduce mdspan and use it everywhere `u_old` and `u_new` are expected.\n",
    "- [`exercise1o.cpp`](./exercise1o.cpp): make the implementation of file io and MPI communication independent from the memory layout, to support changing the layout in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a43a19-d346-48d3-8c20-f3e74e137a0a",
   "metadata": {},
   "source": [
    "## Exercise 1o: Using `std::mdspan` everywhere `u_old` and `u_new` are expected.\n",
    "\n",
    "In this optional exercise, we replace the manual layout and data access computations of Exercise 1 with C++23 multi-dimensional spans `std::mdspan`. To learn more about `std::mdspan` see [A Gentle Introduction to mdspan](https://github.com/kokkos/mdspan/wiki/A-Gentle-Introduction-to-mdspan). \n",
    "\n",
    "In C++23, `std::mdspan` is available in the `<mdspan>` header. It provides non-owning multi-dimensional data access using the multi-dimensional square bracket operator (`array[i, j, k]`), which is another C++23 extension. The implementation provided here is backported to C++20, and the macro `MDSPAN_USE_PAREN_OPERATOR` can be defined to `1` to use the paren `operator()` instead (`array(i, j, k)`) to enable using it in older compilers. As compilers gain support for `operator[]` this macro can be removed.\n",
    "\n",
    "In this exercise we start by including `std::mdspan`:\n",
    "\n",
    "```c++\n",
    "#define MDSPAN_USE_PAREN_OPERATOR 1\n",
    "#include <mdspan>\n",
    "```\n",
    "\n",
    "The current implementation provided in the containers is under the `std::experimental` namespace. This will change once C++23 is ratified and the implementation is made standard conforming. In the meantime, writing `std::experimental::mdspan` is quite cumbersome, so we will define a namespace alias `stdex` and use that instead. Once C++23 is ratified and implementations are updated, we can switch this to `std` instead:\n",
    "\n",
    "```c++\n",
    "namespace stdex = std::experimental;\n",
    "```\n",
    "\n",
    "`mdspan` is customizable through many type parameters:\n",
    "\n",
    "```c++\n",
    "template <typename T, typename Extents, typename Layout, ... /* others */>\n",
    "class mdspan;\n",
    "```\n",
    "\n",
    "* `T`: is the value type of the multi-dimensional array, e.g., `double`.\n",
    "* `Extents`: describe the index type, the number of dimensions, and number of elements per dimension in the array:\n",
    "   ```c++\n",
    "   using grid_extents = stdex::extents<std::size_t, stdex::dynamic_extent, stdex::dynamic_extent>;\n",
    "   ```\n",
    "* `Layout`: the memory layout of the array\n",
    "  * `std::layout_right`: elements are contiguous along the _right most_ index (row-major in 2D).\n",
    "  * `std::layout_left`: elements are contiguous along the _left most_ index (col-major in 2D).\n",
    "\n",
    "Now that we are all set up, we will create a multi-dimensional span with two runtime dimensions indexed with an integer of type `std::size_t`. \n",
    "The dimensions of an [`std::mdspan`] are defined via [`std::extents`] type: \n",
    "\n",
    "```\n",
    "// Two dymensional dynamic extents of type size_t:\n",
    "using grid_extents = stdex::extents<std::size_t, stdex::dynamic_extent, stdex::dynamic_extent>;\n",
    "```\n",
    "\n",
    "Now we need to pick the layout of our `std::mdspan`. The C++23 standard comes with a couple of layouts, two of which are called [`std::layout_right`] and [`std::layout_left`]. Here, \"left\" and \"right\" indicate which dimension is contiguously in memory. For row-major format, the right most dimension, i.e., `y` for our 2D grid, is contiguous in memory, so we set our layout to right:\n",
    "\n",
    "```c++\n",
    "using grid_layout = stdex::layout_right;\n",
    "```\n",
    "\n",
    "And with this we can finally create our two-dimensional `std::mdspan`:\n",
    "\n",
    "```c++\n",
    "using grid_t = stdex::mdspan<double, grid_extent, grid_layout>; \n",
    "```\n",
    "\n",
    "Summary: to create a multi-dimensional span to access our grid, we've had to do:\n",
    "\n",
    "```c++\n",
    "using grid_extents = stdex::extents<std::size_t, stdex::dynamic_extent, stdex::dynamic_extent>;\n",
    "using grid_layout = stdex::layout_right;\n",
    "using grid_t = stdex::mdspan<double, grid_extents, grid_layout>;\n",
    "```\n",
    "\n",
    "Since our goal is to use mdspans for accessing our data, we will now rename our storage for the variables to have a `_data` suffix, and will create multi-dimensional spans to access them:\n",
    "\n",
    "```c++\n",
    "// Allocate memory\n",
    "std::vector<double> u_new_data(p.n()), u_old_data(p.n());\n",
    "\n",
    "grid_t u_new{u_new_data.data(), p.nx+2, p.ny};  // mdspan constructor takes a point to data followed by the extents\n",
    "grid_t u_old{u_old_data.data(), p.nx+2, p.ny};\n",
    "```\n",
    "\n",
    "Before, the type of the `u_old` and `u_new` variables was `double*`, but after the above change it will be an `std::mdspan`.\n",
    "Since these two types are incompatible, the compiler will helpfully emit an error everywhere we were expecting a `double*`, telling us precisely what to update next. We will chase these errors until the mini-application compiles again, and then we will verify the results.\n",
    "\n",
    "We will start by changing a bunch of API signatures from `double*` to `grid_t`:\n",
    "\n",
    "```c++\n",
    "double inner(grid_t u_new, grid_t u_old, parameters p);\n",
    "double prev (grid_t u_new, grid_t u_old, parameters p); \n",
    "double next (grid_t u_new, grid_t u_old, parameters p);\n",
    "```\n",
    "\n",
    "And then continue by implementing our `initial_condition`: \n",
    "\n",
    "```c++\n",
    "void initial_condition(grid_t u_new, grid_t u_old) {\n",
    "  std::fill_n(std::execution::par, u_old.data_handle(), u_old.size(), 0.0);\n",
    "  std::fill_n(std::execution::par, u_new.data_handle(), u_new.size(), 0.0);\n",
    "}\n",
    "```\n",
    "\n",
    "Notice that we have dropped the function argument `n`, and that we continue to use `fill_n` by writing directly to the data behind the `mdspan`.\n",
    "For containers like `std::vector`, a handle to the data can be obtained by just calling the `vec.data()` member function.\n",
    "For `mdspan`, this function is called `mdspan.data_handle()` instead.\n",
    "\n",
    "Then we will continue by updating the pointer passed to MPI File I/O for writing the solution:\n",
    "\n",
    "```c++\n",
    "MPI_File_iwrite_at(f, values_offset, u_old.data_handle() + p.ny, values_per_rank, MPI_DOUBLE, &req[0]);\n",
    "```\n",
    "\n",
    "Finally, we get to update the main function where `std::mdspan` makes a big difference:\n",
    "\n",
    "```c++\n",
    "// Finite-difference stencil\n",
    "double stencil(grid_t u_new, grid_t u_old, long x, long y, parameters p) {\n",
    "   // Update the content\n",
    "}\n",
    "```\n",
    "\n",
    "Until now, we've been accessing elements with:\n",
    "\n",
    "```c++\n",
    "  auto idx = [=](auto x, auto y) { \n",
    "      // Index into the memory using row-major order:\n",
    "      assert(x >= 0 && x < 2 * p.nx);\n",
    "      assert(y >= 0 && y < p.ny);\n",
    "      return x * p.ny + y;\n",
    "  };\n",
    "  u_new[idx(x, y)] = ...;\n",
    "```\n",
    "\n",
    "But now we can access them through the `mdspan` directly:\n",
    "\n",
    "```c++\n",
    "  u_new(x,y) = ...;\n",
    "```\n",
    "\n",
    "And one last thing, we need to update the handles for the MPI calls in `prev` and `next` as well.\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise1o.cpp]: ./exercise1o.cpp\n",
    "\n",
    "While [exercise1o.cpp] compiles and runs correctly as provided, it does not use `std::mdspan` yet. Follow the TODOs in the file to port it to use `std::mdspan` as described in the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7070f1e-1729-4e20-9cda-79c14a01bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lab2_heat/vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582400fb-4d0f-493f-b77b-a30016f346fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise1o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755159-154a-412e-8e1f-7e02dc298d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise1o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c784-f0e9-45ca-b81e-fc8a3cb6c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=multicore -o heat exercise1o.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b76631-b203-43d0-aa4a-760ce90bbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=gpu -o heat exercise1o.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fad2c-aa93-44ac-86b4-4c3e9e5f3b34",
   "metadata": {},
   "source": [
    "### Solutions Exercise 1o\n",
    "\n",
    "The solution for this exercise is in [`solutions/exercise1o.cpp`].\n",
    "\n",
    "[`solutions/exercise1o.cpp`]: ./solutions/exercise1o.cpp\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 1 using different compilers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985569b-aef5-45c0-9e01-6820caa96d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise1o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766ced6-902a-4de9-b762-221a126cf4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise1o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d0e6a-031e-44de-903d-117733b31bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=multicore  -o heat solutions/exercise1o.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6df4b-7d43-4baf-a6f3-349838a36468",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=gpu -o heat solutions/exercise1o.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c649e5-6aa1-4fa5-be4b-e76f5fcaba60",
   "metadata": {},
   "source": [
    "## Exercise 2o: Support varying the `mdspan` layout\n",
    "\n",
    "In the previous exercise we ported our example to use `std::mdspan` everywhere, but that does not mean that our application is correct for any layout.\n",
    "For example, the following change will break our results:\n",
    "\n",
    "```c++\n",
    "// using grid_layout = stdex::layout_right;  \n",
    "using grid_layout = stdex::layout_left;  // Change layout from right to left!\n",
    "```\n",
    "\n",
    "The starting point at [exercise2o.cpp](./exercise2o.cpp) takes the solution of the Exercise 1o and makes this one change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0eb30-3373-444f-bdac-511f110e956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which mpicxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a088d-55f0-4c47-997c-bf59cc2c8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat exercise2o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e0754-2070-4efc-8fce-e8a6089e409f",
   "metadata": {},
   "source": [
    "This solution is obviously not right. What's the problem?\n",
    "\n",
    "The problem is that MPI File I/O usage, MPI communication, and the visualization scripts, all expect `std::layout_right`.\n",
    "\n",
    "We'll solve this problem by:\n",
    "* File I/O: translating from our data-layout to `std::layout_right` before doing File I/O\n",
    "* MPI communication: performing packing and unpacking into vectors of contiguous elements\n",
    "\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise2o.cpp]: ./exercise2o.cpp\n",
    "\n",
    "While [exercise2o.cpp] compiles correctly, it generates incorrect results. \n",
    "Follow the TODOs in the file to fix the issues and obtain correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84bab1-9be1-44d4-8e62-4f8b039fe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise2o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93488481-c954-4586-b91a-cc7e70081eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise2o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6a6d5-1bb6-43d1-bc6c-e7e114eed6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=multicore -o heat exercise2o.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e881b3-b93b-47ff-bd5f-7ef0bf09e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=gpu -o heat exercise2o.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9009670-8ae1-4c61-a372-1e54a1f5d50d",
   "metadata": {},
   "source": [
    "### Solutions Exercise 2o\n",
    "\n",
    "The solution for this exercise is in [`solutions/exercise2o.cpp`].\n",
    "\n",
    "[`solutions/exercise2o.cpp`]: ./solutions/exercise2o.cpp\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 1 using different compilers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109340b-d42e-44f2-a8c3-11ff14da58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise2o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f9a98-ae01-40b0-ad4e-cc561ed6b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise1o.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68130742-43b4-4ea7-b8dd-7bf4019ed8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=multicore -o heat solutions/exercise1o.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579357fd-6a8d-4aa7-9c42-8e46ac5dc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -O4 -fast -march=native -DNDEBUG -stdpar=gpu -o heat solutions/exercise1o.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
