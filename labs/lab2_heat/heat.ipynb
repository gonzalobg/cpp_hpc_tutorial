{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with Standard C++\n",
    "===\n",
    "\n",
    "# Lab 2: 2D Unsteady Heat Equation\n",
    "\n",
    "In this tutorial we will take a pre-existing MPI mini-application and parallelize it using the C++ parallel algorithms to obtain an \"hybrid\" mini-application - hybrid as in MPI & C++ - that runs on multiple nodes using multiple CPU cores or one GPU per rank. \n",
    "\n",
    "A working implementation using only MPI is provided in [starting_point.cpp].\n",
    "\n",
    "[starting_point.cpp]: ./starting_point.cpp\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Let's start by compiling and running the starting point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat starting_point.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a compiler\n",
    "\n",
    "Here we use the `OMPI_CXX` environment variable preceding the `mpicxx` command to pick the C++ compiler, e.g., `OMPI_CXX=g++` picks the GNU g++ compiler.\n",
    "\n",
    "### Number of ranks\n",
    "\n",
    "**PLEASE** be mindful when using the notebooks through one of the systems we provide during the tutorials.\n",
    "These systems are often shared by many students and increasing the number of ranks worsens the experience for everybody.\n",
    "\n",
    "Having said that, feel free to increase the number of ranks when running the tutorial on your own system :)\n",
    "\n",
    "### Program Command Line Interface\n",
    "\n",
    "* `./heat 1024 1024 4000`: runs the heat-equation mini-application; it takes three arguments: `NX NY NITERATIONS`\n",
    "  * `NX` and `NY`: number of unknown per MPI rank in `x` and `y` dimensions\n",
    "  * `NITERATIONS`: number of time-step to simulate\n",
    "  \n",
    "### Performance results\n",
    "\n",
    "This mini-application performs a \"weak scaling\": `NX` and `NY` control the number of unknowns per MPI rank, such that NX=256 NY=256 results in a grid size of 256x256 nodes per rank. When running the application with, e.g., two ranks, the size of the whole grid doubles to 512x256 along the NX dimension (the NY dimension is kept constant).\n",
    "\n",
    "The performance results show two memory BWs: the memory BW delivered by 1 rank, and by all ranks.\n",
    "  \n",
    "### Visualizing the results\n",
    "\n",
    "The `heat` binary writes a solution to an `output` file, that can be converted to a png file using the `visualize()` API provided in the [`vis.py`](./vis.py) file. Let's include the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run vis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and use it to visualize the solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat starting_point.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -o heat starting_point.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallelize with C++ parallel algorithms\n",
    "\n",
    "The goal of this exercise is to parallelize the `stencil` and `initialize` implementations using the C++ parallel algorithms.\n",
    "\n",
    "A template for the solution is provided in [exercise1.cpp].\n",
    "The only functions that needs to be modified to achieve this are the `stencil` and `initial_condition` functions.\n",
    "\n",
    "In the serial implementation, raw loops are used:\n",
    "\n",
    "```c++\n",
    "#include <cartesian_product.hpp> // Brings C++23 std::views::cartesian_product to C++20\n",
    "// TODO: add C++ standard library includes as necessary\n",
    "// #include <...>\n",
    "\n",
    "double stencil(double* u_new, double* u_old, grid g, parameters p) {\n",
    "  // TODO: implement using parallel algorithms\n",
    "  double energy = 0.;\n",
    "  for (long x = g.x_begin; x < g.x_end; ++x) {\n",
    "    for (long y = g.y_begin; y < g.y_end; ++y) {\n",
    "      energy += stencil(u_new, u_old, x, y, p);\n",
    "    }\n",
    "  }\n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "Parallelize it using the C++ parallel algorithms by:\n",
    "* Using `std::views::cartesian_product` to create a range ([documentation](https://en.cppreference.com/w/cpp/ranges/cartesian_product_view)).\n",
    "* Using the `std::transform_reduce` algorithm (overload 3 from the [documentation](https://en.cppreference.com/w/cpp/algorithm/transform_reduce)).\n",
    "\n",
    "While `std::views::cartesian_product` is part of C++23, most C++ toolchains do not provide an implementation of it yet.\n",
    "For this tutorial, we provide a single header implementation of `cartesian_product` that is backported to C++20.\n",
    "The exercise templates include it for you via: \n",
    "\n",
    "```c++\n",
    "#include <cartesian_product.hpp> // Brings C++23 std::views::cartesian_product to C++20\n",
    "```\n",
    "\n",
    "Once C++23 support includes it, you'll automatically get it via the `<ranges>` include.\n",
    "\n",
    "To keep all memory on the device when offloading the C++ parallel algorithms, also parallelize the `initial_condition` function from its raw loop version using the `std::fill_n` ([documentation](https://en.cppreference.com/w/cpp/algorithm/fill_n)) parallel algorithm:\n",
    "\n",
    "```c++\n",
    "void initial_condition(double* u_new, double* u_old, long n) {\n",
    "  // TODO: implement using parallel algorithms\n",
    "  for (long i = 0; i < n; ++i) {\n",
    "    u_old[i] = 0.;\n",
    "    u_new[i] = 0.;\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise1.cpp]: ./exercise1.cpp\n",
    "\n",
    "While [exercise1.cpp] compiles and runs as provided, it produces incorrect results due to the incomplete `stencil` and `initialize` implementations.\n",
    "Search for `TODO`s in the file and fix them until your compiler of choice compiles and run correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise1.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise1.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 1\n",
    "\n",
    "The solution for this exercise is in [`solutions/exercise1.cpp`].\n",
    "\n",
    "[`solutions/exercise1.cpp`]: ./solutions/exercise1.cpp\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 1 using different compilers.\n",
    "\n",
    "**NOTE**: The problem size used here is too small to benefit from GPU acceleration. Try with a problem that's 1GB large, like using the following CLI arguments `65536 128 1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise1.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise1.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=multicore -o heat solutions/exercise1.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `-stdpar=gpu`, nvc++ enables `-gpu=managed` by default to emulate Unified Memory on non-coherent platforms.\n",
    "\n",
    "When combining this with MPI, we can tell UCX-based MPI implementations that Managed Memory used with MPI \"mostly\" resides on the device via the `UCX_RNDV_FRAG_MEM_TYPE=cuda` environment variable, which will force UCX to use device-pinned communication buffers instead of staging data communication via the host CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=gpu -o heat solutions/exercise1.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Overlapping Communication and Computation\n",
    "\n",
    "The goal of this exercise is to overlap communicaiton with computation using `std::thread`, `std::atomic`, and `std::barrier`.\n",
    "\n",
    "A template for the solution is provided in [exercise2.cpp]. \n",
    "[exercise2.cpp]: ./exercise2.cpp\n",
    "\n",
    "First, notice that the computation involves a data exchange with neighbors and is split into three steps:\n",
    "\n",
    "* `internal`: processes internal rows that do not depend on data from neighbors\n",
    "* `prev_boundary`: exchanges data with neighbor at `rank - 1` and processes the rows that depend on the elements received\n",
    "* `next_boundary`: exchanges data with neighbor at `rank + 1` and processes the rows that depend on the elements received\n",
    "\n",
    "\n",
    "```c++\n",
    "double internal(double* u_new, double* u_old, parameters p) {\n",
    "    grid g { .x_start = 2, .x_end = p.nx, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    energy += stencil(u_new.get(), u_old.get(), g, p);\n",
    "}\n",
    "\n",
    "double prev_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    // Send window cells, receive halo cells\n",
    "    if (p.rank > 0) {\n",
    "      // Send bottom boundary to bottom rank\n",
    "      MPI_Send(u_old + p.ny, p.ny, MPI_DOUBLE, p.rank - 1, 0, MPI_COMM_WORLD);\n",
    "      // Receive top boundary from bottom rank\n",
    "      MPI_Recv(u_old + 0, p.ny,  MPI_DOUBLE, p.rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "    grid g { .x_start = p.nx, .x_end = p.nx + 1, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "\n",
    "double next_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    if (p.rank < p.nranks - 1) {\n",
    "        // Receive bottom boundary from top rank\n",
    "        MPI_Recv(u_old + (p.nx + 1) * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        // Send top boundary to top rank, and\n",
    "        MPI_Send(u_old + p.nx * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 1, MPI_COMM_WORLD);\n",
    "    }\n",
    "    grid g { .x_start = 1, .x_end = 2, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "```\n",
    "\n",
    "In the previous exercise, these steps are performed sequentially:\n",
    "\n",
    "```c++\n",
    "for (long it = 0; it < p.nit(); ++it) {\n",
    "    double energy = 0.;\n",
    "    // Exchange and compute domain boundaries:\n",
    "    energy += prev_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += next_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += internal(u_new.data(), u_old.data(), p);\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this exercise, we need to modify the application to perform these three steps concurrently and in parallel.\n",
    "\n",
    "This will require:\n",
    "\n",
    "* using one `std::thread` per computation in such a way that we do not launch one thread on every iteration\n",
    "* using `std::atomic<double>` for the `energy`, to enable the separate threads to modify the energy concurrently\n",
    "* using a `std::barrier` to synchronize the different threads\n",
    "\n",
    "Furthermore, one of the threads will need to perform the following in a critical section:\n",
    "  * `MPI_Reduce` of the `energy`: this operation requires for all threads to have updated the `energy` for the current iteration, so it must happen after these updates have completed\n",
    "  * reset the `energy` to `0.` before the next iteration: all threads must wait for this operation to complete before starting the next iteration\n",
    "  \n",
    "The template [exercise2.cpp] provides `TODO`s to guide you through this process: \n",
    "\n",
    "```c++\n",
    "  // TODO: use an atomic variable for the energy\n",
    "  double energy = 0.;\n",
    "    \n",
    "  // TODO: use a barrier for synchronization\n",
    "  // ...bar = ...\n",
    "\n",
    "  // TODO: use threads for the different computations\n",
    "  auto thread_prev = std::thread([/*TODO: complete capture */]() {\n",
    "      for (long it = 0; it < p.nit(); ++it) {\n",
    "          // TODO: perform the prev exchange and computation\n",
    "          // TODO: update the atomic energy\n",
    "          // TODO: synchronize with the barrier\n",
    "      }\n",
    "  });\n",
    "    \n",
    "  auto thread_next = /* TODO: similar for prev */;\n",
    "      \n",
    "  auto thread_internal = /*\n",
    "    TODO: same as for next and prev\n",
    "    TODO: need to perform the reduction in one of the threads (for example this one)\n",
    "    TODO: need to reset the atomic in one of the threads (for example this one)\n",
    "  */;\n",
    "\n",
    "  // TODO: join all threads\n",
    "\n",
    "```\n",
    "\n",
    "[exercise2.cpp]: ./exercise2.cpp\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "\n",
    "The following commands compile but produce incorrect results.\n",
    "Your goal is to fix that by following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise2.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise2.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=gpu -o heat exercise2.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 2\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise2.cpp` sub-directory.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 2 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise2.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise2.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=multicore -o heat solutions/exercise2.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=gpu -o heat solutions/exercise2.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Senders & Receivers\n",
    "\n",
    "The goal of this exercise is to simplify the implementation of Exercise 2 - Overlap Communication and Computation - by using Senders & Receivers with a `static_thread_pool` to manage the host threads, while combining this with the C++ parallel algorithms.\n",
    "\n",
    "The implementation of Exercise 2 is quite complex. It requires:\n",
    "\n",
    "```c++\n",
    "// A shared atomic variable to accumulate the energy:\n",
    "std::atomic<double> energy = 0.;\n",
    "\n",
    "// A shared barrier for synchronizing threads:\n",
    "std::barrier bar(3);\n",
    "\n",
    "// User must manually create and start threads:\n",
    "std::thread thread_inner(..[&] {\n",
    "      energy += computation(...);\n",
    "      bar.arrive_and_wait();\n",
    "      // User must manually create a critical section for MPI rank reduction: \n",
    "      MPI_Reduce(...);\n",
    "      // User must manually reset the shared state on each iteration:\n",
    "      energy = 0;\n",
    "      bar.arrive_and_wait();\n",
    "  });\n",
    "\n",
    "std::thread thread_prev(...);\n",
    "std::thread thread_next(...);\n",
    "\n",
    "// User must manually join all threads before doing File I/O\n",
    "thread_prev.join();\n",
    "thread_next.join();\n",
    "thread_inner.join();\n",
    "\n",
    "// File I/O\n",
    "```\n",
    "\n",
    "In this exercise, we'll use Senders & Receivers instead to create a graph representing the computation:\n",
    "\n",
    "```c++\n",
    "stde::sender iteration_step(stde::scheduler sch, parameters p, long it,\n",
    "                            std::vector<double>& u_new, std::vector<double>& u_old) {\n",
    "    // TODO: use Senders & Receivers to create a graph representing the computation of a single iteration   \n",
    "}\n",
    "```\n",
    "\n",
    "and will then dispatch it to an execution context:\n",
    "\n",
    "```c++\n",
    "stde::static_thread_pool ctx{3}; // Thread Pool with 3 threads\n",
    "stde::scheduler auto sch = ctx.get_scheduler();\n",
    "\n",
    "for (long it = 0; it < p.nit(); ++it) {\n",
    "    stde::sync_wait(iteration_step(sch));\n",
    "}\n",
    "```\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise3.cpp]: ./exercise3.cpp\n",
    "\n",
    "The template [exercise3.cpp] compiles and runs as provided, but produces incorrect results due to the incomplete `iteration_step` implementation.\n",
    "\n",
    "After completing it the following blocks should compile and run correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise3.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -o heat exercise3.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=multicore -o heat exercise3.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -stdpar=gpu -o heat exercise3.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 3\n",
    "\n",
    "The solutions for each example are available in the [`solutions/exercise3.cpp`] sub-directory.\n",
    "\n",
    "[`solutions/exercise3.cpp`]: ./solutions/exercise3.cpp\n",
    "\n",
    "The following blocks compiles and runs the solutions for Exercise 3 using different compilers and C++ standard versions.\n",
    "By default, the [`static_thread_pool`] scheduler is used.\n",
    "\n",
    "[`static_thread_pool`]: https://github.com/NVIDIA/stdexec/blob/main/include/exec/static_thread_pool.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise3.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -o heat solutions/exercise3.cpp -ltbb\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=multicore -o heat solutions/exercise3.cpp\n",
    "!mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -std=c++20 -Ofast -march=native -DNDEBUG -stdpar=gpu -o heat solutions/exercise3.cpp\n",
    "!UCX_RNDV_FRAG_MEM_TYPE=cuda mpirun -np 2 ./heat 256 256 16000\n",
    "visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
