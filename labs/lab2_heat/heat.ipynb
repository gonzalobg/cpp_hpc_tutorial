{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with Standard C++\n",
    "===\n",
    "\n",
    "# Lab 2: 2D Unsteady Heat Equation\n",
    "\n",
    "In this tutorial we will learn how to do multi-dimensional iteration in C++17 and C++23 and how to integrate parallel algorithms with pre-existing MPI applications, by accelerating a 2D heat equation solver (see slides).\n",
    "\n",
    "A working implementation is provided in [starting_point.cpp].\n",
    "Please take 5 minutes to skim through it.\n",
    "\n",
    "[starting_point.cpp]: ./starting_point.cpp\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Let's start by compiling and running the starting point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat starting_point.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 4 ./heat 256 256 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "* MPI implementation is `OpenMPI`, the environment variable `OMPI_CXX` selects the C++ compiler to be used, e.g., `OMPI_CXX=g++` uses GCC\n",
    "* Need to use the `mpicxx` compiler-wrapper to compile\n",
    "* Because we are running in a container, we need to disable some MPI features with: `OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root`\n",
    "* The binary invocation itself is: `./heat 1024 1024 4000`, the binary takes three arguments: NX NY NITERATIONS , that is, the number of unknowns in x and y dimensions, and the number of iterations to compute.\n",
    "\n",
    "The binary writes a solution to an `output` file, that can be converted to a png file using the `vis` script or the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('dark_background') # Uncomment for dark background\n",
    "\n",
    "def visualize(name = 'output'):\n",
    "    f = open(name, 'rb')\n",
    "    grid = np.fromfile(f, dtype=np.uint64, count=2, offset=0)\n",
    "\n",
    "    nx = grid[0]\n",
    "    ny = grid[1]\n",
    "\n",
    "    times = np.fromfile(f, dtype=np.float64, count=1, offset=0)\n",
    "    time = times[0]\n",
    "\n",
    "    values = np.fromfile(f, dtype=np.float64, offset=0)\n",
    "    assert len(values) == nx * ny, f'{len(values)} != {nx * ny}'\n",
    "    values = values.reshape((nx, ny))\n",
    "\n",
    "    print(f'Plotting grid {nx}x{ny}, t = {time}')\n",
    "\n",
    "    plt.title(f'Temperature at t = {time:.3f} [s]')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.pcolormesh(values, cmap=plt.cm.jet, vmin=0, vmax=values.max())\n",
    "    plt.colorbar()\n",
    "    plt.savefig('output.png', transparent=True, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: parallelize with C++ parallel algorithms\n",
    "\n",
    "The goal of this exercise is to parallelize the `stencil` and `initialize` implementations using the C++ parallel algorithms.\n",
    "\n",
    "A template for the solution is provided in [exercise0.cpp].\n",
    "The only functions that needs to be modified to achieve this are the `stencil` and `initialize` functions.\n",
    "\n",
    "In the serial implementation, raw loops are used:\n",
    "\n",
    "```c++\n",
    "double stencil(double *u_new, double *u_old, grid g, parameters p) {\n",
    "  double energy = 0.;\n",
    "  for (long x = g.x_start; x < g.x_end; ++x) {\n",
    "    for (long y = g.y_start; y < g.y_end; ++y) {\n",
    "      energy += stencil(u_new, u_old, x, y, p);\n",
    "    }\n",
    "  }\n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "The teaching materials cover two methods for approaching multi-dimensional iteration that can be used here:\n",
    "\n",
    "* linear indexing\n",
    "* `views::cartesian_product`\n",
    "\n",
    "### Linear indexing\n",
    "\n",
    "When exploring \"linear indexing\", notice that there is already a function called `index` in the file, which maps 2D indices to 1D indices:\n",
    "\n",
    "```c++\n",
    "// Index into the memory using row-major order:\n",
    "long index(long x, long y, parameters p) {\n",
    "    assert(x >= 0 && x < p.nx);\n",
    "    assert(y >= 0 && y < p.ny);\n",
    "    return x * p.ny + y;\n",
    "};\n",
    "```\n",
    "\n",
    "one needs to create a `split` function, e.g., here:\n",
    "\n",
    "```c++\n",
    "double stencil(double *u_new, double *u_old, grid g, parameters p) {\n",
    "  double energy = 0.;\n",
    "  // TODO: implement using parallel algorithms\n",
    "  \n",
    "  auto split = [/* TODO: captures */](long idx) -> std::pair<long, long> {\n",
    "  \n",
    "  };\n",
    "  \n",
    "  // ...TODO...\n",
    "  \n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "Recall that the goal for `split` and `index` is for the following invariant to hold:\n",
    "\n",
    "```c++\n",
    "auto [x1, y1] = split(index(x0, y0, p), p);\n",
    "assert(x0 == x1 && y0 == y1);\n",
    "```\n",
    "\n",
    "### `views::cartesian_product`\n",
    "\n",
    "Since `views::cartesian_product` is not part of C++17 or C++20, one needs to include the range-v3 library to use them.\n",
    "\n",
    "Please feel encouraged to explore using `views::cartesian_product` in this exercise.\n",
    "Subsequent exercises will use linear indexing since it has better compiler support today.\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise0.cpp]: ./exercise0.cpp\n",
    "\n",
    "While [exercise0.cpp] compiles and runs as provided, it produces incorrect results due to the incomplete `stencil` and `initialize` implementations.\n",
    "Search for `TODO`s in the file and fix them until your compiler of choice compiles and run correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise0.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise0.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat exercise0.cpp\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 0\n",
    "\n",
    "The solutions for this first exercise are availale here:\n",
    "\n",
    "* [solutions/exercise0.cpp] uses linear indexing\n",
    "* [solutions/exercise0_cartesian.cpp] uses `views::cartesian_product`\n",
    "\n",
    "[solutions/exercise0.cpp]: ./solutions/exercise0.cpp\n",
    "[solutions/exercise0.cpp]: ./solutions/exercise0_cartesian.cpp\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 0 using different compilers and C++ standard versions.\n",
    "\n",
    "#### Exercise 0: linear indexing solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise0.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise0.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat solutions/exercise0.cpp\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('output_nvc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 0: `views::cartesian_product` solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++17 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise0_cartesian.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=clang++ mpicxx -std=c++17 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise0.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 0: pinned device memory with `thrust::device_vector` solution\n",
    "\n",
    "To benefit from NVLink, a solution using `thrust::device_vector` to allocate device-pinned memory is provided.\n",
    "\n",
    "This is will no longer be necessary in the next HPC SDK release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat solutions/exercise0_nomanaged.cpp\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: overlapping computation with communication\n",
    "\n",
    "The goal of this exercise is to overlap communicaiton with computation using `std::thread`, `std::atomic`, and `std::barrier`.\n",
    "\n",
    "A template for the solution is provided in [exercise1.cpp]. \n",
    "[exercise1.cpp]: ./exercise1.cpp\n",
    "\n",
    "First, notice that the computation involves a data exchange with neighbors and is split into three steps:\n",
    "\n",
    "* `internal`: processes internal rows that do not depend on data from neighbors\n",
    "* `prev_boundary`: exchanges data with neighbor at `rank - 1` and processes the rows that depend on the elements received\n",
    "* `next_boundary`: exchanges data with neighbor at `rank + 1` and processes the rows that depend on the elements received\n",
    "\n",
    "\n",
    "```c++\n",
    "double internal(double* u_new, double* u_old, parameters p) {\n",
    "    grid g { .x_start = 2, .x_end = p.nx, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    energy += stencil(u_new.get(), u_old.get(), g, p);\n",
    "}\n",
    "\n",
    "double prev_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    // Send window cells, receive halo cells\n",
    "    if (p.rank > 0) {\n",
    "      // Send bottom boundary to bottom rank\n",
    "      MPI_Send(u_old + p.ny, p.ny, MPI_DOUBLE, p.rank - 1, 0, MPI_COMM_WORLD);\n",
    "      // Receive top boundary from bottom rank\n",
    "      MPI_Recv(u_old + 0, p.ny,  MPI_DOUBLE, p.rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "    grid g { .x_start = p.nx, .x_end = p.nx + 1, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "\n",
    "double next_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    if (p.rank < p.nranks - 1) {\n",
    "        // Receive bottom boundary from top rank\n",
    "        MPI_Recv(u_old + (p.nx + 1) * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        // Send top boundary to top rank, and\n",
    "        MPI_Send(u_old + p.nx * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 1, MPI_COMM_WORLD);\n",
    "    }\n",
    "    grid g { .x_start = 1, .x_end = 2, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "```\n",
    "\n",
    "In the previous exercise, these steps are performed sequentially:\n",
    "\n",
    "```c++\n",
    "for (long it = 0; it < p.nit(); ++it) {\n",
    "    double energy = 0.;\n",
    "    // Exchange and compute domain boundaries:\n",
    "    energy += prev_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += next_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += internal(u_new.data(), u_old.data(), p);\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this exercise, we need to modify the application to perform these three steps concurrently and in parallel.\n",
    "\n",
    "This will require:\n",
    "\n",
    "* using one `std::thread` per computation in such a way that we do not launch one thread on every iteration\n",
    "* using `std::atomic<double>` for the `energy`, to enable the separate threads to modify the energy concurrently\n",
    "* using a `std::barrier` to synchronize the different threads\n",
    "\n",
    "Furthermore, one of the threads will need to perform the following in a critical section:\n",
    "  * `MPI_Reduce` of the `energy`: this operation requires for all threads to have updated the `energy` for the current iteration, so it must happen after these updates have completed\n",
    "  * reset the `energy` to `0.` before the next iteration: all threads must wait for this operation to complete before starting the next iteration\n",
    "  \n",
    "The template [exercise1.cpp] provides `TODO`s to guide you through this process: \n",
    "\n",
    "```c++\n",
    "  // TODO: use an atomic variable for the energy\n",
    "  double energy = 0.;\n",
    "    \n",
    "  // TODO: use a barrier for synchronization\n",
    "  // ...bar = ...\n",
    "\n",
    "  // TODO: use threads for the different computations\n",
    "  auto thread_prev = std::thread([/*TODO: complete capture */]() {\n",
    "      for (long it = 0; it < p.nit(); ++it) {\n",
    "          // TODO: perform the prev exchange and computation\n",
    "          // TODO: update the atomic energy\n",
    "          // TODO: synchronize with the barrier\n",
    "      }\n",
    "  });\n",
    "    \n",
    "  auto thread_next = /* TODO: similar for prev */;\n",
    "      \n",
    "  auto thread_internal = /*\n",
    "    TODO: same as for next and prev\n",
    "    TODO: need to perform the reduction in one of the threads (for example this one)\n",
    "    TODO: need to reset the atomic in one of the threads (for example this one)\n",
    "  */;\n",
    "\n",
    "  // TODO: join all threads\n",
    "\n",
    "```\n",
    "\n",
    "[exercise1.cpp]: ./exercise1.cpp\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "\n",
    "The following commands compile but produce incorrect results.\n",
    "Your goal is to fix that by following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise1.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80,nomanaged -std=c++20 -fast -DNDEBUG -o heat exercise1.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 1\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise1.cpp` sub-directory.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 3 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise1.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat solutions/exercise1.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
